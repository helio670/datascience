{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NASA](http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg)\n",
    "\n",
    "<center><h1><font size=\"+3\">Introductory Dask Training</font></h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ You will need the _graphviz_ package installed if you want to see the dask graph visualizations. I would recommend using the _pip_ installer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "1. What is Dask?\n",
    "2. A simple example.\n",
    "3. Using Dask with Bokeh server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Dask?\n",
    "\n",
    "---\n",
    "\n",
    "It is simply a Python library/package to add parallel computing capabilities. It primarily is constructed of two parts (I would consider 3, but the documentation says 2):\n",
    "\n",
    "- __Dynamic task scheduler:__ this is the worker that once Dask has a task graph created, this portion handles the execution on parallel hardware whether it be a single machine or a cluster\n",
    "- __Big data collections:__ think the usefulness of NumPy and Pandas APIs but used within Dask\n",
    "- Task graphs: Dask's underlying step that connects the data collections and task schedulers are these graphs that Dask creates before using the functionality of Pandas or scheduling a task through the scheduler.\n",
    "\n",
    "Relevant links:\n",
    "* [https://dask.org/](https://dask.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Dask?\n",
    "\n",
    "---\n",
    "\n",
    "Python, in general, is a great tool for data scientists. However, packages used by these scientists such as NumPy, Pandas, and so forth were not designed for computation beyond a single machine. Dask takes the usefulness of these packages and extends them to be used across multiple machines for larger applications, computations and analysis while still being able to scale back down to be used on a single machine. Dask also maintains it's familiarity of these packages for those that have used them before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrays implement the Numpy API\n",
    "import dask.array as da\n",
    "x = da.random.random(size=(10000, 10000),\n",
    "                     chunks=(1000, 1000))\n",
    "x + x.T - x.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes implement the Pandas API\n",
    "import dask\n",
    "df = dask.datasets.timeseries()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.groupby(df.id).x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A simple example\n",
    "\n",
    "---\n",
    "\n",
    "Here, we will go beyond just calculating in series and try to use Dask's distributed module to do calculations in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import random\n",
    "\n",
    "def inc(x):\n",
    "    sleep(0.2)\n",
    "    return x + 1\n",
    "\n",
    "def double(x):\n",
    "    sleep(0.2)\n",
    "    return 2 * x\n",
    "\n",
    "def add(x,y):\n",
    "    sleep(0.2)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serial/Sequential calculations to obtain a mathematical total. (Benchmark ~5 seconds on a Mac laptop.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1\n",
    "\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "out = []\n",
    "for x in data:\n",
    "    y = inc(x)\n",
    "    z = double(y)\n",
    "    out.append(z)\n",
    "    \n",
    "total = 0\n",
    "for z in out:\n",
    "    total = add(total, z)\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update\n",
    "\n",
    "> We tell dask in the code below that each of these methods are going to be parallelized by wrapping them as dask delayed functions. The functions have not been executed in this block below, merely proxy objects that have a graph associated with it (see visualize below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "# delayed means to setup but not compute yet\n",
    "inc = dask.delayed(inc)\n",
    "double = dask.delayed(double)\n",
    "add = dask.delayed(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update\n",
    "\n",
    "> The `visualize` method below requires the _graphviz_ package. I have tried installing it via `conda install`, but what worked for me was `pip install` on the command line in the appropriate Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = inc(1)\n",
    "y = inc(2)\n",
    "z = add(x, y)\n",
    "dask.visualize(z, rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update\n",
    "\n",
    "> The next code block now uses our wrapped/delayed methods to create a delayed total. It has not physically calculated the total in which makes this execution very fast. The actual computation follows afterwards via the `dask.compute` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "out = []\n",
    "for x in data:\n",
    "    y = inc(x)\n",
    "    z = double(y)\n",
    "    out.append(z)\n",
    "    \n",
    "total = 0\n",
    "for z in out:\n",
    "    total = add(total, z)\n",
    "    \n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update\n",
    "\n",
    "> Now we have our new delayed methods that will be parallelized rather than sequential. To schedule or perform the processes/calculations we tell dask to compute the total. You will notice that it is faster (~2 seconds)than the prior sequential computation of roughly ~5 seconds on a Mac laptop.\n",
    "\n",
    "> It also returns the correct calculated value as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1\n",
    "dask.compute(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that even though this was parallelized, there are still parts that are sequential in computation. This hints at better parallelization algorithms/schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(total, rankdir='LR') # sequential dependence still evident by visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "out = []\n",
    "for x in data:\n",
    "    y = inc(x)\n",
    "    z = double(y)\n",
    "    out.append(z)\n",
    "    \n",
    "# tree reduction\n",
    "while len(out) > 1:\n",
    "    out = [add(out[i], out[i+1]) for i in range(0, len(out), 2)]\n",
    "\n",
    "total = out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(total, rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update\n",
    "\n",
    "> While this new method of the tree reduction seems fast, remember that it has not actually performed the calculation in question. Let's do that now.\n",
    "\n",
    "> I saw a reduction in time to ~1 second with this new and more appropriate algorithm for summations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1\n",
    "\n",
    "dask.compute(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can instantly see which of these two algorithms will be better for parallelization. But what if we had a more complex computation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "# 15x15 array of ones chunked into 5x5 squares (uses NumPy mainly)\n",
    "x = da.ones((15, 15), chunks=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize((x.dot(x.T) - x.mean(axis=0)).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this graph does not give us any indication whether our parallelization is efficient or not (as well as using resources appropriately). We need a way to visualize how it __performs__ on a system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Dask with Bokeh Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update\n",
    "\n",
    "Steps to create the dask bokeh server:\n",
    "1. You need 2 terminal/command prompt sessions/windows that have the appropriate Python environment activated/sourced.\n",
    "2. Execute `dask-scheduler` in the first session/window. If you do not have this command line utilty, you need to install the _\"distributed\"_ package. [Link](http://distributed.readthedocs.io/en/latest/install.html)\n",
    "3. The scheduler will give addresses:\n",
    "  * scheduler: this is the address we use for the workers\n",
    "  * http: to view the local web server, copy this address into a browser\n",
    "  * bokeh: this is where our bokeh server/visualizer/dashboard will be\n",
    "4. Schedule a dask worker for the scheduler in the next terminal session/window:\n",
    "```bash\n",
    "dask-worker xxx.xxx.xxx.xxx:xxxx\n",
    "```\n",
    "By using the address given from the scheduler (minus the tcp part).\n",
    "5. Change the code below in the jupyter notebook to reflect the dask-scheduler address.\n",
    "6. Open a new browser tab/window and go to the bokeh server address from the __dask-scheduler__ information. If you used the one from the worker, you will not have all the utilities available.\n",
    "7. Click on the __status__ text to view the dashboard for processes running.\n",
    "8. Execute the code below from the notebook and view the output in the dask bokeh dashboard. You can repeatedly do this and modify the parallelization to get better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client\n",
    "from time import sleep\n",
    "import random\n",
    "import dask\n",
    "\n",
    "def inc(x):\n",
    "    sleep(random.random() / 10)\n",
    "    return x + 1\n",
    "\n",
    "def dec(x):\n",
    "    sleep(random.random() / 10)\n",
    "    return x - 1\n",
    "\n",
    "def add(x, y):\n",
    "    sleep(random.random() / 10)\n",
    "    return x + y\n",
    "\n",
    "\n",
    "# change this to the address and port from your dask bokeh server\n",
    "client = Client('128.154.190.87:8786')\n",
    "\n",
    "incs = client.map(inc, range(100))\n",
    "decs = client.map(dec, range(100))\n",
    "adds = client.map(add, incs, decs)\n",
    "total = client.submit(sum, adds)\n",
    "\n",
    "del incs, decs, adds\n",
    "total.result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
