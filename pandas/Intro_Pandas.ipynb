{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NASA](http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg)\n",
    "\n",
    "<center><h1><font size=\"+3\">GSFC Introductory Python Training</font></h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/pandas_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Reference: http://pandas.pydata.org</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics:\n",
    "    1. Pandas data structures\n",
    "    2. Loading data\n",
    "    3. Cleaning and formatting data\n",
    "    4. Basic visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The <font color='red'>Pandas</font> library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print('Using pandas version ',pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# only 10 rows of data will be displayed\n",
    "pd.set_option(\"max_rows\", 10) \n",
    "# print floating point numbers using fixed point notation,\n",
    "np.set_printoptions(suppress=True)\n",
    "LARGE_FIGSIZE = (8, 6) # set figure size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from seaborn import set_style\n",
    "#set_style(\"darkgrid\")\n",
    "import seaborn as sns\n",
    "sns.set(style='ticks', context='talk')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Pandas data structures</center>\n",
    "\n",
    "<center>Pandas data structures are similar to numpy ndarrays but with extra functionality.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D data structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A <font color='red'>Series</font>  is a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). The axis labels are collectively referred to as the index. \n",
    "\n",
    "Think of a Series as a cross between a list and a dict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/pandas_series.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A series can be constructed with the `pd.Series` constructor (passing a list or array of values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = [5, 8, 13, 0.1, -5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(my_list)   # Numpy\n",
    "print(type(a))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(my_list) # Pandas\n",
    "print(type(s))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...get default index values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NumPy arrays as backend of Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.values  # Contains an array of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s.index # has an associated array of data labels from 0, N-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More on the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(my_list,\n",
    "        index=['A','B','C','D','E'])\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy Array has an implicitly defined integer index used to access the values while the Pandas Series has an explicitly defined index associated with the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# value at position n in series\n",
    "print(s[3])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use iloc to get value at position n\n",
    "print(s.iloc[3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value at given index\n",
    "print(s.loc['D'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### index and position are not the same !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2D data structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas: <font color='red'>DataFrame</font> is a 2-dimensional labeled data structure with columns of potentially different types. It is generally the most commonly used pandas object.\n",
    "\n",
    "A <font color='red'>DataFrame</font> is like a sequence of aligned <font color='red'>Series</font> objects, i.e. they share the same index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/pandas_df.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DataFrame can be thought of as a generalization of a two-dimensional NumPy array, where both the rows and columns have a generalized index for accessing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data=[[5, True, 'x', 2.7],\n",
    "          [8, True, 'y', 3.1],\n",
    "          [13,False,'z',np.NaN],\n",
    "          [1, False, 'a', 0.1],\n",
    "          [-5, True, 'b', -2]],\n",
    "    index=['A','B','C','D','E'],\n",
    "    columns=['num', 'bool', 'str', 'real']\n",
    ")\n",
    "print(type(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a DataFrame from a 2D Numpy array\n",
    "\n",
    "Given a two-dimensional array of data, we can create a dataframe with any specified column and index names. If left out, an integer index will be used for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_array = np.random.rand(3, 2)\n",
    "print(my_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(my_array,\n",
    "             columns=['foo', 'bar'],\n",
    "             index=['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Weather data</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>https://www.wunderground.com/cgi-bin/findweather/getForecast?query=KDAA</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head data/weather/hampton_10-10-15_10-10-16.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas <font color='red'>read_csv</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = pd.read_csv(\"data/weather/hampton_10-10-15_10-10-16.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic information on the data\n",
    "weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Access values of a column like in a dictionary \n",
    "weather_data[\"Max TemperatureF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can get the column index first and the values\n",
    "my_col = weather_data.columns.get_loc(\"Max TemperatureF\")\n",
    "weather_data.iloc[:,my_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use the loc method\n",
    "weather_data.loc[:,\"Max TemperatureF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access column data like a \"method\"\n",
    "# nicer because you can autocomplete\n",
    "weather_data.EDT  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select multiple columns\n",
    "weather_data[[\"EDT\", \"Mean TemperatureF\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also pass an argument\n",
    "weather_data.EDT.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data[\"Mean TemperatureF\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename columns\n",
    "\n",
    "Assign a new list of column names to the columns property of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.columns = [\"date\", \"max_temp\", \"mean_temp\", \"min_temp\", \"max_dew\",\n",
    "                \"mean_dew\", \"min_dew\", \"max_humidity\", \"mean_humidity\",\n",
    "                \"min_humidity\", \"max_pressure\", \"mean_pressure\",\n",
    "                \"min_pressure\", \"max_visibilty\", \"mean_visibility\",\n",
    "                \"min_visibility\", \"max_wind\", \"mean_wind\", \"min_wind\",\n",
    "                \"precipitation\", \"cloud_cover\", \"events\", \"wind_dir\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can use . dot \n",
    "weather_data.mean_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.mean_temp.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weather_data.mean_temp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weather_data[['max_temp','min_temp']].plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weather_data = weather_data[['max_temp','min_temp']]\n",
    "new_weather_data.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can specify column labels in the loc method to retrieve \n",
    "# columns by label instead of by position\n",
    "new_weather_data = weather_data.loc[50:125,['max_temp','min_temp']]\n",
    "new_weather_data.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <font color='red'>plot()</font> function returns a matplotlib <font color='red'>AxesSubPlot</font> object. You can pass this object into subsequent calls to plot() in order to compose plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = weather_data.max_temp.plot(title=\"Min and Max Temperatures\", \n",
    "                                figsize=(12,6))\n",
    "weather_data.min_temp.plot(style=\"red\", ax=ax)\n",
    "ax.set_ylabel(\"Temperature (F)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Climate data</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Global Surface Temperature Change based on Land and Ocean Data</center>\n",
    "<center>Reference http://pubs.giss.nasa.gov/docs/2010/2010_Hansen_ha00510u.pdf</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!head data/temperatures/annual.land_ocean.90S.90N.df_1880-2016mean.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas  <font color='red'>read_table</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/temperatures/annual.land_ocean.90S.90N.df_1880-2016mean.dat\"\n",
    "tsurf = pd.read_table(filename)\n",
    "print(type(tsurf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tsurf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only 1 column! Let's reformat the data noting that there is a header and values are separated by any number of spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "''Data Wrangling is the process of converting and mapping data from its raw form to another format with the purpose of making it more valuable and appropriate for advance tasks such as Data Analytics and Machine Learning.''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tsurf = pd.read_table(filename, skiprows=5, sep=\"\\s+\")\n",
    "tsurf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are columns but the column names are: 1880, -0.20, -0.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsurf = pd.read_table(filename, skiprows=5, sep=\"\\s+\", names=[\"year\", \"mean_anom\", \"with_smoothing\"])\n",
    "tsurf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have 3 columns, one of which is the year of the record. Let use that as the index using the `index_col` option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsurf = pd.read_table(filename, skiprows=5, sep=\"\\s+\", names=[\"year\", \"mean_anom\", \"with_smoothing\"], \n",
    "                                index_col=0)\n",
    "tsurf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore the index is made of dates. Let's make that explicit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsurf = pd.read_table(filename, skiprows=5, sep=\"\\s+\", names=[\"year\", \"mean_anom\", \"with_smoothing\"], \n",
    "                                index_col=0, parse_dates=True)\n",
    "tsurf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsurf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to missing values to NaN values\n",
    "tsurf[tsurf == -999.000] = np.nan\n",
    "tsurf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove NaN values\n",
    "tsurf.dropna().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tsurf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = tsurf.mean_anom.plot(style=\"black\", \n",
    "                          title=\"Global Mean Estimates based on Land and Ocean Data\", \n",
    "                          marker='s',\n",
    "                          figsize=(12,6))\n",
    "tsurf.with_smoothing.plot(style=\"red\", ax=ax)\n",
    "ax.set_ylabel(\"Temperature (C)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://data.giss.nasa.gov/gistemp/graphs/graph_data/Global_Mean_Estimates_based_on_Land_and_Ocean_Data/graph.html'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Exercise </font>\n",
    "* Read the weather data so that the indixes are the dates\n",
    "* Plot the max and min tempatures on the same axes with the dates (ranging from November to March) as x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Sea level dataset</center>\n",
    "\n",
    "The university of colorado posts updated timeseries for mean sea level globally, per hemisphere, and even per ocean, sea, ... Let's download the global one, and the ones for the northern and southern hemisphere.\n",
    "\n",
    "That will also illustrate that to load text files that are online, there is no more work than replacing the filepath by a URL in `read_table`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "northern_sea_level = pd.read_table(\"http://sealevel.colorado.edu/files/current/sl_nh.txt\", \n",
    "                                   sep=\"\\s+\")\n",
    "print(type(northern_sea_level))\n",
    "northern_sea_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "southern_sea_level = pd.read_table(\"http://sealevel.colorado.edu/files/current/sl_sh.txt\", \n",
    "                                   sep=\"\\s+\")\n",
    "southern_sea_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 2016 version of the global dataset:\n",
    "url = \"http://sealevel.colorado.edu/files/2016_rel2/sl_ns_global.txt\"\n",
    "global_sea_level = pd.read_table(url, sep=\"\\s+\")\n",
    "global_sea_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new DataFrames\n",
    "\n",
    "As shown before `DataFrames` can  be created manually by grouping several Series together. Let's make a new frame from the 3 sea level datasets we downloaded above. They will be displayed along the same index. \n",
    "\n",
    "Wait, does it make sense to do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For two Series to share the same DataFrame the Series have to be aligned.\n",
    "# Let's look at sea level data for NH and SH. Are they aligned?\n",
    "southern_sea_level.year == northern_sea_level.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could use Numpy:\n",
    "np.all(southern_sea_level.year == northern_sea_level.year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the northern hemisphere and southern hemisphere datasets are aligned. What about the global one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(global_sea_level.year))\n",
    "print(len(northern_sea_level.year))\n",
    "len(global_sea_level.year) == len(northern_sea_level.year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's just build a DataFrame with the 2 hemisphere datasets then. We will come back to add the global one later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary of Series\n",
    "mean_sea_level = pd.DataFrame({\"northern_hem\": northern_sea_level[\"msl_ib(mm)\"], \n",
    "                               \"southern_hem\": southern_sea_level[\"msl_ib(mm)\"], \n",
    "                               \"date\": northern_sea_level.year})\n",
    "mean_sea_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still the date in a regular column and a numerical index that is not that meaningful (or useful). We can specify the `index` of a `DataFrame` at creation. Let's try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sea_level = pd.DataFrame({\"northern_hem\": northern_sea_level[\"msl_ib(mm)\"], \n",
    "                               \"southern_hem\": southern_sea_level[\"msl_ib(mm)\"]},\n",
    "                               index = northern_sea_level.year)\n",
    "mean_sea_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that\n",
    "northern_sea_level[\"msl_ib(mm)\"].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no `value` corresponding to the Series' `index`.\n",
    "\n",
    "But there is `value` corresponding to the specified `index`.\n",
    "\n",
    "So, replace the Series by their values when creating the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_sea_level = pd.DataFrame({\"northern_hem\": northern_sea_level[\"msl_ib(mm)\"].values, \n",
    "                               \"southern_hem\": southern_sea_level[\"msl_ib(mm)\"].values},\n",
    "                               index = northern_sea_level.year)\n",
    "mean_sea_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index name, `year`, is not an accurate description of what it indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rename an index by setting its name. \n",
    "\n",
    "For example, the index of the `mean_sea_level` dataFrame could be called `date` since it contains more than just the year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sea_level.index.name = \"date\"\n",
    "mean_sea_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While building the `mean_sea_level` dataFrame earlier, we didn't include the values from `global_sea_level` since the years were not aligned. Adding a column to a dataframe is as easy as adding an entry to a dictionary. So let's try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sea_level[\"mean_global\"] = global_sea_level[\"msl_ib_ns(mm)\"]\n",
    "mean_sea_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column is full of NaNs again because the auto-alignment feature of Pandas is searching for the index values like `1992.9323` in the index of `global_sea_level[\"msl_ib_ns(mm)\"]` series and not finding them. Let's set its index to these years so that that auto-alignment can work for us and figure out which values we have and not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_sea_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_sea_level = global_sea_level.set_index(\"year\")\n",
    "global_sea_level[\"msl_ib_ns(mm)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_sea_level[\"mean_global\"] = global_sea_level[\"msl_ib_ns(mm)\"]\n",
    "mean_sea_level.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"max_rows\", 40):\n",
    "    print(mean_sea_level.fillna(value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sea_level.plot(subplots=True, figsize=(16, 12));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more plot options inside `pandas.tools.plotting`; for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sea_level.plot(kind='kde', figsize=(12, 8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there correlations between the northern and southern sea level timeseries we loaded?\n",
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(mean_sea_level, figsize=LARGE_FIGSIZE);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Web scraping sea level data</center>\n",
    "\n",
    "There is more data about mean sea levels. For example, the PSMSL website (http://www.psmsl.org/) contains MSL data from stations around the world. Here we download and parse all tables in a webpage, and again we just give `read_html` the URL to parse:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas  <font color='red'>read_html</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs `lxml`, `beautifulSoup4` and `html5lib` python packages\n",
    "table_list = pd.read_html(\"http://www.psmsl.org/data/obtaining/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(table_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is 1 table on that page which contains metadata about the stations where \n",
    "# sea levels are recorded\n",
    "local_sea_level_stations = table_list[0]\n",
    "local_sea_level_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That table can be used to search for a station in a region of the world we choose, extract an ID for it and download the corresponding time series with the URL http://www.psmsl.org/data/obtaining/met.monthly.data/< ID >.metdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets that we obtain straight from the reading functions are pretty raw. A lot of pre-processing can be done during data read but we haven't used all the power of the reading functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns of the local_sea_level_stations aren't clean.\n",
    "# they contain spaces and dots.\n",
    "local_sea_level_stations.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clean them up a bit:\n",
    "local_sea_level_stations.columns = [name.strip().replace(\".\", \"\") \n",
    "                                    for name in local_sea_level_stations.columns]\n",
    "local_sea_level_stations.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Global temperature climatology</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a different file with temperature data. NASA's GISS dataset is written in chunks: look at it in `data/temperatures/GLB.Ts+dSST.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head data/temperatures/GLB.Ts+dSST.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><font size=\"+3\" color=\"red\">NEED to UPDATE</font></h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "giss_temp = pd.read_csv(\"data/temperatures/GLB.Ts+dSST.txt\", sep=\"\\t\")\n",
    "type(giss_temp)\n",
    "giss_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal nature of the object\n",
    "print(giss_temp.shape)\n",
    "print(giss_temp.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptors for the vertical axis (axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(giss_temp.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptors for the horizontal axis (axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giss_temp.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall: every column is a Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of information at once including memory usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giss_temp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We didn't set a column number of the index of giss_temp, \n",
    "# we can do that after we have read the data:\n",
    "giss_temp = giss_temp.set_index(\"Year\")\n",
    "giss_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note Year.1 column is redundant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giss_temp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop it:\n",
    "giss_temp = giss_temp.drop(\"Year.1\", axis=1) # axis=1 is the data axis\n",
    "giss_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also just select the columns we want to keep (another way to drop columns)\n",
    "giss_temp = giss_temp[[u'Jan', u'Feb', u'Mar', u'Apr', u'May', u'Jun', u'Jul', \n",
    "                       u'Aug', u'Sep', u'Oct', u'Nov', u'Dec']]\n",
    "# Note how we passed a List of column names\n",
    "\n",
    "giss_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove the last row (Year  Jan ...).\n",
    "giss_temp = giss_temp.drop(\"Year\")  # by  default drop() works on index axis (axis=0)\n",
    "giss_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also set `****` to a real missing value (`np.nan`). We can often do it using a boolean mask, but that may trigger pandas warning. Another way to assign based on a boolean condition is to use the `where` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#giss_temp[giss_temp == \"****\"] = np.nan # WARNING due to memory layout\n",
    "\n",
    "# use .where\n",
    "giss_temp = giss_temp.where(giss_temp != \"****\", np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giss_temp.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the labels (strings) found in the middle of the timeseries, every column only assumed to contain strings (didn't convert them to floating point values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giss_temp.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That can be changed after the fact (and after the cleanup) with the `astype` method of a `Series`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giss_temp[\"Jan\"].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all columns that had 'Object' type and make them 'float32'\n",
    "for col in giss_temp.columns:\n",
    "    giss_temp[col] = giss_temp[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An index has a `dtype` just like any Series and that can be changed after the fact too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giss_temp.index.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's change it to an integer so that values can at least be compared properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giss_temp.index = giss_temp.index.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will remove any year that has a missing value. Use how='all' to keep partial years\n",
    "giss_temp.dropna(how=\"all\").tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace (fill) NaN with 0 (or some other value, like -999)\n",
    "giss_temp.fillna(value=0).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffill = forward fill: This fills them with the previous year.\n",
    "giss_temp.fillna(method=\"ffill\").tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a `.interpolate` method that works on a `Series`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giss_temp.Aug.interpolate().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will leave the missing values in all our datasets, because it wouldn't be meaningful to fill them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giss_temp.Jan.plot(figsize=LARGE_FIGSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A boxplot\n",
    "giss_temp.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Import netCDF file to Pandas dataframe</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas can read HDF5 directly, but not netCDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><font size=\"+3\" color=\"red\">NEED to UPDATE</font></h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename ='/Users/ccruz/data/nws/nws_precip_conus_20160115.nc'\n",
    "vname = 'amountofprecip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = netCDF4.Dataset(filename)\n",
    "precip = nc.variables[vname]\n",
    "print(precip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip.plot()   # This will not work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter xarray\n",
    "Reference: http://xarray.pydata.org/en/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip = xr.open_dataset(filename)\n",
    "print(precip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip[vname].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Storing our work</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each `read_**` function to load data, there is a `to_**` method attached to Series and DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another file format that is commonly used is Excel.\n",
    "\n",
    "Multiple datasets can be stored in 1 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(\"test.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giss_temp.to_excel(writer, sheet_name=\"GISS temp data\")\n",
    "tsurf.to_excel(writer, sheet_name=\"NASA sst anom data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another, more powerful file format to store binary data, which allows us to store both `Series` and `DataFrame`s without having to cast anybody is HDF5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.HDFStore(\"all_data.h5\") as writer:\n",
    "    giss_temp.to_hdf(writer, \"/temperatures/giss\")\n",
    "    tsurf.to_hdf(writer, \"/temperatures/anomalies\")\n",
    "    mean_sea_level.to_hdf(writer, \"/sea_level/mean_sea_level\")\n",
    "    local_sea_level_stations.to_hdf(writer, \"/sea_level/stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Extra material\n",
    "\n",
    "### Bulk operations\n",
    "\n",
    "Methods like sum() and std() work on entire columns. We can run our own functions across all values in a column (or row) using apply()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.date.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the values property of the column to get a list of values for the column. Inspecting the first value reveals that these are strings with a particular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_date = weather_data.date.values[0]\n",
    "first_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the <font color='red'>strptime</font> function from the <font color='red'>datetime</font> module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "dt = datetime.strptime(first_date, \"%Y-%m-%d\")\n",
    "print(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the <font color='red'>apply()</font> method, which takes an <font color='blue'>anonymous function</font>, we can apply strptime to each value in the column. We'll overwrite the string date values with their Python datetime equivalents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.date = weather_data.date.apply(lambda d: datetime.strptime(d, \"%Y-%m-%d\"))\n",
    "weather_data.date.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go one step futher. Each row in our DateFrame represents the weather from a single day. Each row in a DataFrame is associated with an index, which is a label that uniquely identifies a row.\n",
    "\n",
    "Our row indices up to now have been auto-generated by pandas, and are simply integers from 0 to 365. If we use dates instead of integers for our index, we will get some extra benefits from pandas when plotting later on. Overwriting the index is as easy as assigning to the <font color='red'>index</font> property of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.index = weather_data.date\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can quickly look up a row by its date with the <font color='red'>ix[]</font> property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.ix[datetime(2015, 10, 10)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.precipitation.plot(figsize=LARGE_FIGSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.max_temp.tail().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.max_temp.tail().plot(kind=\"bar\", rot=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <font color='red'>plot()</font> function returns a matplotlib <font color='red'>AxesSubPlot</font> object. You can pass this object into subsequent calls to plot() in order to compose plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = weather_data.max_temp.plot(title=\"Min and Max Temperatures\")\n",
    "weather_data.min_temp.plot(style=\"red\", ax=ax)\n",
    "ax.set_ylabel(\"Temperature (F)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
